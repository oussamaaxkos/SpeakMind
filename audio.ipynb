{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca8327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-3.5.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7153d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d334987d7f24829a6cf422f243ea70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\datasets--google--Synthetic-Persona-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7077fc74fe4692a1ef907785f27e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Synthetic-Persona-Chat_train.csv:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c423bac7ef45e583c34bae5f499b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Synthetic-Persona-Chat_valid.csv:   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce26fba9f094fe6ab69701c4c8dc6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Synthetic-Persona-Chat_test.csv:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962c2b1c8b7640598137df961db7e0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263fd4ac976746d2b59a755fe5e36520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb01ff1c6bb453fbe438166fa6edd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     user 1 personas  \\\n",
      "0  I am 32.\\nI do not want a job.\\nI play video g...   \n",
      "1  I am 32.\\nI play video games all day.\\nI still...   \n",
      "2  I am 32.\\nI play video games all day.\\nI still...   \n",
      "3  I write.\\nI work at mcdonald s.\\nI watch youtu...   \n",
      "4  I am bald.\\nI like to swim.\\nMy favorite drink...   \n",
      "\n",
      "                                     user 2 personas  \\\n",
      "0  My favorite drink is iced coffee.\\nI have a bl...   \n",
      "1  I have a ford f150.\\nI like ford cars.\\nMy tru...   \n",
      "2  I can recite the movie young frankenstein word...   \n",
      "3  I want to move.\\nI don t like feeling controll...   \n",
      "4  My favorite store is american eagle.\\nI enjoy ...   \n",
      "\n",
      "                         Best Generated Conversation  \n",
      "0  User 1: Hi! I'm [user 1's name].\\nUser 2: Hi [...  \n",
      "1  User 1: Hey, how's it going?\\nUser 2: Good, I'...  \n",
      "2  User 1: Hi, my name is John. What's your name?...  \n",
      "3  User 1: Hi!\\nUser 2: Hey!\\nUser 1: What's up?\\...  \n",
      "4  User 1: Hello!\\nUser 2: Hi!\\nUser 1: What do y...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from Hugging Face Hub\n",
    "dataset = load_dataset(\"google/Synthetic-Persona-Chat\")\n",
    "\n",
    "# Convert to DataFrames if needed\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_val = dataset[\"validation\"].to_pandas()\n",
    "df_test = dataset[\"test\"].to_pandas()\n",
    "\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example row:\n",
      "{'user 1 personas': 'I am 32.\\nI do not want a job.\\nI play video games all day.\\nI still live at home with my parents.', 'user 2 personas': 'My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.', 'Best Generated Conversation': \"User 1: Hi! I'm [user 1's name].\\nUser 2: Hi [user 1's name], I'm [user 2's name].\\nUser 1: What do you do for fun?\\nUser 2: I like to play video games, go to the beach, and read.\\nUser 1: I like to play video games too! I'm not much of a reader, though.\\nUser 2: What video games do you like to play?\\nUser 1: I like to play a lot of different games, but I'm really into competitive online games right now.\\nUser 2: I'm not really into competitive games, I like to play more relaxing games.\\nUser 1: That's cool. What kind of relaxing games do you like to play?\\nUser 2: I like to play puzzle games, simulation games, and story-based games.\\nUser 1: I've never been much of a puzzle game person, but I do like simulation games and story-based games.\\nUser 2: Nice! What's your favorite simulation game?\\nUser 1: I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun.\\nUser 2: I've heard good things about that game. I might have to check it out.\\nUser 1: You should! It's a lot of fun.\\nUser 2: Well, I'm glad we met. Maybe we can play some games together sometime.\\nUser 1: That would be fun!\\nUser 2: Great! I'll send you my Steam name.\\nUser 1: Ok, sounds good.\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"google/Synthetic-Persona-Chat\")\n",
    "\n",
    "# Use the train split for quick testing\n",
    "data = dataset[\"train\"]\n",
    "\n",
    "# Show an example conversation\n",
    "print(\"Example row:\")\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d649ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: my money is in the wallet\n",
      "Bot: Alright, I will remember that.\n",
      "\n",
      "You: where did I put my money?\n",
      "Bot: You put your my money in the the wallet.\n"
     ]
    }
   ],
   "source": [
    "class MemoryBot:\n",
    "    def __init__(self):\n",
    "        self.memory = {}\n",
    "\n",
    "    def tell(self, statement):\n",
    "        # Basic parsing like: \"my money is in the wallet\"\n",
    "        if \" is in \" in statement:\n",
    "            parts = statement.split(\" is in \")\n",
    "            item = parts[0].strip().lower()\n",
    "            location = parts[1].strip().lower()\n",
    "            self.memory[item] = location\n",
    "            return \"Alright, I will remember that.\"\n",
    "        return \"Okay.\"\n",
    "\n",
    "    def ask(self, question):\n",
    "        # Basic question parsing\n",
    "        for item in self.memory:\n",
    "            if item in question.lower():\n",
    "                return f\"You put your {item} in the {self.memory[item]}.\"\n",
    "        return \"I'm not sure.\"\n",
    "\n",
    "# ----------------------------\n",
    "# Test it\n",
    "bot = MemoryBot()\n",
    "\n",
    "# Tell the bot something\n",
    "print(\"You: my money is in the wallet\")\n",
    "print(\"Bot:\", bot.tell(\"my money is in the wallet\"))\n",
    "\n",
    "# Ask the bot a question\n",
    "print(\"\\nYou: where did I put my money?\")\n",
    "print(\"Bot:\", bot.ask(\"where did I put my money?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993288cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"google/Synthetic-Persona-Chat\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Combine persona + history into one input string\n",
    "def preprocess(example):\n",
    "    input_text = f\"{example['persona']} [SEP] {' '.join(example['history'])}\"\n",
    "    target_text = example['reply']\n",
    "    full_text = input_text + \" \" + target_text\n",
    "    tokens = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=256)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized = dataset[\"train\"].map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Training setup\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./dialogpt-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcaa241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./dialogpt-finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Chat loop\n",
    "chat_history_ids = None\n",
    "\n",
    "print(\"ðŸ¤– Trained Chatbot is ready. Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
    "\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=1000,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c52b7",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30efbc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "# 1. Load dataset\n",
    "dataset = load_dataset(\"google/Synthetic-Persona-Chat\")\n",
    "\n",
    "# 2. Split dataset into train and eval (80/20 split)\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# 3. Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4. Preprocessing function\n",
    "def preprocess(example):\n",
    "    # Get the conversation turns\n",
    "    conversation = example[\"Best Generated Conversation\"]\n",
    "    \n",
    "    # Join all turns except the last one as context\n",
    "    context = \"\\n\".join(conversation[:-1])\n",
    "    # The last turn is the response we want to predict\n",
    "    response = conversation[-1]\n",
    "    \n",
    "    # Tokenize context and response\n",
    "    model_inputs = tokenizer(\n",
    "        context,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Tokenize response for labels\n",
    "    labels = tokenizer(\n",
    "        text_target=response,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 5. Tokenize datasets\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess,\n",
    "    batched=False,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess,\n",
    "    batched=False,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "# 6. Training arguments - with better saving configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dialoGPT-finetuned\",  # Changed to more specific directory\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_safetensors=True,  # Save in safe format\n",
    "    load_best_model_at_end=True,  # Will load best model at end\n",
    "    metric_for_best_model=\"eval_loss\",  # Use eval loss to determine best\n",
    "    greater_is_better=False,  # Lower eval loss is better\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"dialoGPT-persona-chat\"\n",
    ")\n",
    "\n",
    "# 7. Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 8. Train and save\n",
    "trainer.train()\n",
    "\n",
    "# 9. Save the final model and tokenizer\n",
    "final_output_dir = \"./dialoGPT-finetuned-final\"\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "print(f\"Model saved to {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d263e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
